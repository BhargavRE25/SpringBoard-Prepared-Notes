{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Websites\n",
    "\n",
    "* Forbes.com\n",
    "* CNBC.com\n",
    "* Cointelegraph.com\n",
    "* Bitcoin.com\n",
    "* Coindesk.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several sources for news relating to cryptocurrencies. However when extracting meaningful data, it must come from a meaningful reputable source. As such, the 5 websites listed above were selected to extract quality data. Cointelegraph, Bitcoin, and Coindesk were the highested rated cryptocurrency specific news sources. Conversely, Forbes and CNBC are in their own right reputable news sources, not specifically related to the cryptocurrency world. They are incorporated in order to give a well rounded data source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turned off SSL verification so Python will throw a bunch of warngings - Turn off those warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a class to help scrape websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "import requests\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "class parse_web_page:\n",
    "    #constructor\n",
    "    def __init__(self,url,website):\n",
    "        try:\n",
    "            #Make a request to the url and read in the webpage, set SSL verificaiton to False (not transmitting sensitive info)\n",
    "            self.html = requests.get(url, verify = False).text\n",
    "            #Make soup!\n",
    "            self.soup = BeautifulSoup(self.html, 'html.parser')\n",
    "            #Extract all text from the website\n",
    "            self.text = self.soup.get_text()\n",
    "            #Set flags depending on which website is being processed\n",
    "            if website == 'forbes':\n",
    "                self.flag = 1\n",
    "            if website == 'cnbc':\n",
    "                self.flag = 2\n",
    "            if website == 'cointelegraph':\n",
    "                self.flag = 3\n",
    "            if website == 'bitcoin':\n",
    "                self.flag = 4\n",
    "            if website == 'coindesk':\n",
    "                self.flag = 5\n",
    "        except:\n",
    "            print('Please enter a valid URL')\n",
    "            \n",
    "    def page_numbers(self):\n",
    "        #ONLY FOR FORBES SITES - Forbes websites has multiple pages for their articles\n",
    "        #Extract how many pages there are for an article\n",
    "        try:\n",
    "            if self.flag is 1:\n",
    "                #Example of Match Pattern: 'Page 1 / 3'\n",
    "                #Search for pattern and return last value as total pages for the article\n",
    "                total_pages = re.findall('Page\\s[0-9]\\s/\\s([0-9])',self.text)[0]\n",
    "                return int(total_pages)\n",
    "            else:\n",
    "                return None\n",
    "        except:\n",
    "            return None\n",
    "        \n",
    "    def get_headline(self):\n",
    "        #Extract the headline of the article\n",
    "        #Headlines for Coindesk articles are under the <title> tag\n",
    "        if self.flag is 5:\n",
    "            headline = self.soup.title.text\n",
    "        #Headlines for all other websites are under the <h1> tag\n",
    "        else:\n",
    "            headline = self.soup.h1.text\n",
    "        return headline\n",
    "    \n",
    "    def date_and_time(self):\n",
    "        #Forbes date and time\n",
    "        if self.flag == 1:\n",
    "            date = re.search('[A-Za-z]{3}\\s[0-9]+,\\s[0-9]{4}\\s@\\s[0-9]+:[0-9]{2}\\s[A-Z]{2}',self.text)\n",
    "            date_time_object = date.group().split('@')\n",
    "            date_time = datetime.strptime(''.join(date_time_object).replace(',',''), '%b %d %Y  %I:%M %p')\n",
    "            return date_time\n",
    "        #CNBC date and time\n",
    "        if self.flag == 2:\n",
    "            date_time_object = self.soup.time.text.split()\n",
    "            order = [6,5,-1,1,2]\n",
    "            date_time_list = [date_time_object[i] for i in order]\n",
    "            date_time = datetime.strptime(' '.join(date_time_list),'%b %d %Y %I:%M %p')\n",
    "            return date_time\n",
    "        #Cointelegraph date and time\n",
    "        if self.flag == 3:\n",
    "            tag = self.soup.find('div', class_ = 'date')\n",
    "            date_time_object = tag['datetime']\n",
    "            date_time = datetime.strptime(date_time_object, '%Y-%m-%d %H:%M:%S')\n",
    "            return date_time\n",
    "        #Bitcoin and Coindesk date and time\n",
    "        if self.flag == 4 or 5:\n",
    "            tag = self.soup.time\n",
    "            date_time_object = tag['datetime'].replace('T',' ').split('+')[0]\n",
    "            date_time = datetime.strptime(date_time_object,'%Y-%m-%d %H:%M:%S')\n",
    "            return date_time\n",
    "\n",
    "    def raw_text(self):\n",
    "        #Extract all <p> tags from articles - actual text of each website\n",
    "        p_tags = self.soup.find_all('p')\n",
    "        #create a list of all found <p> tags\n",
    "        text = [item.text for item in p_tags]\n",
    "        #Join all <p> tags to reconstruct the paragraphs of the articles\n",
    "        return(' '.join(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an a function that will read in a URL, retrieve that URL, scrape the HTML and write the appropriate data to a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from tqdm import tqdm\n",
    "\n",
    "def scrape_articles(urls, coin, website = None, path = 'C://Users//simskel//Desktop//scraped_data.csv'):\n",
    "    \n",
    "    \n",
    "    #Closure function ONLY FOR FORBES SITES - Used to extract all other pages in the forbes articles and concat them all\n",
    "    def get_other_pages(url,num_of_pages,page_1,website):\n",
    "        lst = [page_1]\n",
    "        list_of_pages = list(range(num_of_pages+1))[2:]\n",
    "        url_of_pages = [''.join([url,str(page)]) for page in list_of_pages]\n",
    "        for url in url_of_pages:\n",
    "            web_page = parse_web_page(url,website)\n",
    "            text = web_page.raw_text()\n",
    "            lst.append(text)\n",
    "        return ''.join(lst)\n",
    "\n",
    "    \n",
    "    with open(path, 'w', newline='', encoding = 'utf-8') as file:\n",
    "        #Create a csv writer object\n",
    "        csv_writer = csv.writer(file)\n",
    "        #Write the headers to the csv file\n",
    "        csv_writer.writerow(['Date','Coin','website','Headline','Text','Link'])\n",
    "        \n",
    "        for url in tqdm(urls):\n",
    "            try:\n",
    "                if website is None:\n",
    "                    #If not given explicitly, extract the website from the url address\n",
    "                    #Used for all except Bitcoin and Cointelegraph\n",
    "                    website = url.split('.')[1]\n",
    "                #Create web page object from class above\n",
    "                page = parse_web_page(url,website)\n",
    "                #Get paragraphs of the article\n",
    "                text = page.raw_text()\n",
    "                #Get date and time of the article\n",
    "                date_time = page.date_and_time()\n",
    "                #Get the headline of the article\n",
    "                headline = page.get_headline()\n",
    "                #Get total number of pages in the article - FORBES ONLY\n",
    "                num_of_pages = page.page_numbers()\n",
    "                \n",
    "                #Get other Forbes pages and concat them\n",
    "                if num_of_pages is not None:\n",
    "                    text = get_other_pages(url,num_of_pages,text,website)\n",
    "                #Write values to csv file\n",
    "                csv_writer.writerow([date_time, coin, website, headline, text, url])\n",
    "            except:\n",
    "                print('Could Not Process: \\n{}'.format(url))\n",
    "        return print('Done Writing File')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From each website listed above, a list of URLs to various articles of interest from each site was composed. These URLS will now be fed through \"scrape_articles\" function in order to be scraped and the resulting data written to CSV files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a list of the Forbes.com links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('Ripple_Forbes.txt') as fh:\n",
    "    file = fh.readlines()\n",
    "    urls_Forbes = [link.strip() for link in file]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a list of the CNBC.com links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('Ripple_CNBC.txt') as fh:\n",
    "    file = fh.readlines()\n",
    "    urls_CNBC = [link.strip() for link in file]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a list of the Coindesk.com links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('Ripple_coindesk.txt') as fh:\n",
    "    file = fh.readlines()\n",
    "    urls_Coindesk = [link.strip() for link in file]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a list of the Cointelegraph.com links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('Ripple_cointelegraph.txt') as fh:\n",
    "    file = fh.readlines()\n",
    "    urls_Cointelegraph = [link.strip() for link in file]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a list of the Bitcoin.com links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('Ripple_bitcoin.txt') as fh:\n",
    "    file = fh.readlines()\n",
    "    urls_Bitcoin = [link.strip() for link in file]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create CSV of all Forbes.com articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 44/44 [00:51<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Writing File\n"
     ]
    }
   ],
   "source": [
    "scrape_articles(urls_Forbes, 'Ripple', path = 'C:\\\\Users\\\\simskel\\\\Desktop\\\\Springboard-Data-Science-Immersive\\\\Capstone 1 Project\\\\Data\\\\Ripple_Forbes.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create CSV of all CNBC.com articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|██████████████▉                            | 9/26 [00:10<00:20,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could Not Process: \n",
      "https://www.cnbc.com/2017/07/21/ripples-xrp-digital-currency-rose-3977-percent-in-the-first-half-of-2017.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|█████████████████████████▊                | 16/26 [00:16<00:10,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could Not Process: \n",
      "https://www.cnbc.com/2017/09/11/ripple-ceo-brad-garlinghouse-on-bitcoin-and-xrp.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 26/26 [00:25<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Writing File\n"
     ]
    }
   ],
   "source": [
    "scrape_articles(urls_CNBC, 'Ripple', path = 'C:\\\\Users\\\\simskel\\\\Desktop\\\\Springboard-Data-Science-Immersive\\\\Capstone 1 Project\\\\Data\\\\Ripple_CNBC.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create CSV of all Coindesk.com articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 47/47 [00:27<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Writing File\n"
     ]
    }
   ],
   "source": [
    "scrape_articles(urls_Coindesk, 'Ripple', path = 'C:\\\\Users\\\\simskel\\\\Desktop\\\\Springboard-Data-Science-Immersive\\\\Capstone 1 Project\\\\Data\\\\Ripple_coindesk.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create CSV of all Cointelegraph.com articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 88/88 [02:11<00:00,  1.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Writing File\n"
     ]
    }
   ],
   "source": [
    "scrape_articles(urls_Cointelegraph, 'Ripple', website = 'cointelegraph', path = 'C:\\\\Users\\\\simskel\\\\Desktop\\\\Springboard-Data-Science-Immersive\\\\Capstone 1 Project\\\\Data\\\\Ripple_cointelegraph.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create CSV of all Bitcoin.com articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 16/16 [00:15<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Writing File\n"
     ]
    }
   ],
   "source": [
    "scrape_articles(urls_Bitcoin, 'Ripple', website = 'bitcoin', path = 'C:\\\\Users\\\\simskel\\\\Desktop\\\\Springboard-Data-Science-Immersive\\\\Capstone 1 Project\\\\Data\\\\Ripple_bitcoin.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
